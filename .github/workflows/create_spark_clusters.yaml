name: Create Dataproc Cluster

on:
  workflow_dispatch:

jobs:
  create-dataproc:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Create JSON credentials file
      uses: jsdaniell/create-json@v1.2.3
      with:
        name: "gcloud-service-key.json"
        json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

    - name: Set up gcloud CLI
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
        export_default_credentials: true

    - name: Read config and deploy to GCS
      run: |
        echo "Reading second bucket info from config..."
        
        BUCKET=$(jq -r '.buckets[1].name' bucket_config.json)
        FOLDER=$(jq -r '.buckets[1].folders[0]' bucket_config.json)

    - name: Create Dataproc Cluster
      run: |
        gcloud dataproc clusters create rakshaka \
          --region=us-central1 \
          --zone=us-central1-a \
          --master-machine-type=e2-standard-2 \
          --master-boot-disk-size=50GB \
          --num-workers=3 \
          --worker-machine-type=e2-standard-2 \
          --worker-boot-disk-size=50GB \
          --image-version=2.1-debian11 \
          --initialization-actions=gs://BUCKET/FOLDER/dataproc_init.sh \
          --optional-components=JUPYTER \
          --enable-component-gateway
