name: Submit Dataproc Spark Job

on:
  workflow_dispatch:

jobs:
  submit-job:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Create JSON credentials file
      id: create-json-credentials
      uses: jsdaniell/create-json@v1.2.3
      with:
          name: "gcloud-service-key.json"
          json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
    
    - name: install the gcloud cli
      uses: google-github-actions/setup-gcloud@v1
      with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          export_default_credentials: true

    - name: Authenticate gcloud CLI explicitly
      run: |
            gcloud auth activate-service-account --key-file=gcloud-service-key.json
            gcloud config set project ${{ secrets.GCP_PROJECT_ID }}

    - name: Install jq
      run: sudo apt-get install -y jq

    - name: Read config and submit Spark job to Dataproc
      run: |
        INGEST_BUCKET=$(jq -r '.ingest_bucket' ingest_run_config.json)
        INGEST_FOLDER=$(jq -r '.ingest_folder' ingest_run_config.json)
        META_BUCKET=$(jq -r '.meta_bucket' ingest_run_config.json)
        META_FOLDER=$(jq -r '.meta_folder' ingest_run_config.json)
        DATASET_ID=$(jq -r '.dataset_id' bq_config.json)

        CONFIG_BUCKET=$(jq -r '.config_bucket' dedup_config.json)
        CONFIG_FOLDER=$(jq -r '.config_folder' dedup_config.json)
        DELTA_BUCKET=$(jq -r '.delta_bucket' dedup_config.json)
        DELTA_FOLDER=$(jq -r '.delta_folder' dedup_config.json)
        QUARANTINE_BUCKET=$(jq -r '.quarantine_bucket' dedup_config.json)
        QUARANTINE_GOOD_FOLDER=$(jq -r '.quarantine_good_folder' dedup_config.json)
        QUARANTINE_BAD_FOLDER=$(jq -r '.quarantine_bad_folder' dedup_config.json)
        PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}

        gcloud dataproc jobs submit pyspark gs://dedup_b/spark_init/spark_main.py \
          --cluster=rakshaka \
          --region=${{ secrets.GCP_REGION }} \
          --py-files = gs://dedup_b/spark_init/spark_utility.py, gs://dedup_b/spark_init/logger_spark.py
          -- \
          --meta_bucket=$META_BUCKET \
          --meta_folder=$META_FOLDER \
          --ingest_bucket=$INGEST_BUCKET \
          --ingest_folder=$INGEST_FOLDER \
          --config_bucket=$CONFIG_BUCKET \
          --config_folder=$CONFIG_FOLDER \
          --delta_bucket=$DELTA_BUCKET \
          --delta_folder=$DELTA_FOLDER \
          --quarantine_bucket=$QUARANTINE_BUCKET \
          --quarantine_good_folder=$QUARANTINE_GOOD_FOLDER \
          --quarantine_bad_folder=$QUARANTINE_BAD_FOLDER \
          --project_id=$PROJECT_ID \
          --dataset_id=$DATASET_ID
